{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file illustrates how you might experiment with the HMM interface.\n",
    "You can paste these commands in at the Python prompt, or execute `test_en.py` directly.\n",
    "A notebook interface is nicer than the plain Python prompt, so we provide\n",
    "a notebook version of this file as `test_en.ipynb`, which you can open with\n",
    "`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import TaggedCorpus\n",
    "from eval import eval_tagging, model_cross_entropy, viterbi_error_rate\n",
    "from hmm import HiddenMarkovModel\n",
    "from crf import ConditionalRandomField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.root.setLevel(level=logging.INFO)\n",
    "log = logging.getLogger(\"test_en\")       # For usage, see findsim.py in earlier assignment.\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch working directory to the directory where the data live.  You may need to edit this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(entrain)=8064  len(ensup)=4051  len(endev)=996\n"
     ]
    }
   ],
   "source": [
    "entrain = TaggedCorpus(Path(\"ensup\"), Path(\"enraw\"))                               # all training\n",
    "ensup =   TaggedCorpus(Path(\"ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n",
    "print(f\"{len(entrain)=}  {len(ensup)=}  {len(endev)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 95936 tokens from ensup\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 12466 word types\n",
      "INFO : Tagset: f['W', 'J', 'N', 'C', 'V', 'I', 'D', ',', 'M', 'P', '.', 'E', 'R', '`', \"'\", 'T', '$', ':', '-', '#', 'S', 'F', 'U', 'L', '_EOS_TAG_', '_BOS_TAG_']\n"
     ]
    }
   ],
   "source": [
    "known_vocab = TaggedCorpus(Path(\"ensup\")).vocab    # words seen with supervised tags; used in evaluation\n",
    "log.info(f\"Tagset: f{list(entrain.tagset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an HMM.  Let's do some pre-training to approximately maximize the\n",
    "regularized log-likelihood on supervised training data.  In other words, the\n",
    "probabilities at the M step will just be supervised count ratios.\n",
    "\n",
    "On each epoch, you will see two progress bars: first it collects counts from\n",
    "all the sentences (E step), and then after the M step, it evaluates the loss\n",
    "function, which is the (unregularized) cross-entropy on the training set.\n",
    "\n",
    "The parameters don't actually matter during the E step because there are no\n",
    "hidden tags to impute.  The first M step will jump right to the optimal\n",
    "solution.  The code will try a second epoch with the revised parameters, but\n",
    "the result will be identical, so it will detect convergence and stop.\n",
    "\n",
    "We arbitrarily choose λ=1 for our add-λ smoothing at the M step, but it would\n",
    "be better to search for the best value of this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Hidden Markov Model (HMM)\n",
      "  0%|          | 0/4051 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m hmm \u001b[38;5;241m=\u001b[39m HiddenMarkovModel(entrain\u001b[38;5;241m.\u001b[39mtagset, entrain\u001b[38;5;241m.\u001b[39mvocab)  \u001b[38;5;66;03m# randomly initialized parameters  \u001b[39;00m\n\u001b[1;32m      3\u001b[0m loss_sup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m model: model_cross_entropy(model, eval_corpus\u001b[38;5;241m=\u001b[39mensup)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mhmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_sup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mλ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mensup_hmm.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HW6/EN.601.465-HW6/code/hmm.py:206\u001b[0m, in \u001b[0;36mHiddenMarkovModel.train\u001b[0;34m(self, corpus, loss, λ, tolerance, max_steps, save_path)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Smooth the counts by a tiny amount to avoid a problem where the M\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# step gets transition probabilities p(t | s) = 0/0 = nan for\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# context tags s that never occur at all, in particular s = EOS.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# multiplied by 0 and added into a sum.  A summand of 0 * nan would\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# regrettably turn the entire sum into nan.      \u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()      \u001b[38;5;66;03m# mark start of training     \u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m dev_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m              \u001b[38;5;66;03m# evaluate the model at the start of training\u001b[39;00m\n\u001b[1;32m    207\u001b[0m old_dev_loss: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m dev_loss     \u001b[38;5;66;03m# loss from the last epoch\u001b[39;00m\n\u001b[1;32m    208\u001b[0m steps: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m   \u001b[38;5;66;03m# total number of sentences the model has been trained on so far      \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Hidden Markov Model (HMM)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m hmm \u001b[38;5;241m=\u001b[39m HiddenMarkovModel(entrain\u001b[38;5;241m.\u001b[39mtagset, entrain\u001b[38;5;241m.\u001b[39mvocab)  \u001b[38;5;66;03m# randomly initialized parameters  \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m loss_sup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m model: \u001b[43mmodel_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_corpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m hmm\u001b[38;5;241m.\u001b[39mtrain(corpus\u001b[38;5;241m=\u001b[39mensup, loss\u001b[38;5;241m=\u001b[39mloss_sup, λ\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      5\u001b[0m           save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensup_hmm.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/HW6/EN.601.465-HW6/code/eval.py:33\u001b[0m, in \u001b[0;36mmodel_cross_entropy\u001b[0;34m(model, eval_corpus)\u001b[0m\n\u001b[1;32m     31\u001b[0m token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gold \u001b[38;5;129;01min\u001b[39;00m tqdm(eval_corpus, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(eval_corpus)):\n\u001b[0;32m---> 33\u001b[0m     logprob \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_corpus\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     34\u001b[0m     token_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gold) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m    \u001b[38;5;66;03m# count EOS but not BOS\u001b[39;00m\n\u001b[1;32m     35\u001b[0m cross_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlogprob \u001b[38;5;241m/\u001b[39m token_count\n",
      "File \u001b[0;32m/etc/conda/envs/nlp-class/lib/python3.9/site-packages/typeguard/__init__.py:1033\u001b[0m, in \u001b[0;36mtypechecked.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m memo \u001b[38;5;241m=\u001b[39m _CallMemo(python_func, _localns, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m   1032\u001b[0m check_argument_types(memo)\n\u001b[0;32m-> 1033\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     check_return_type(retval, memo)\n",
      "File \u001b[0;32m~/Desktop/HW6/EN.601.465-HW6/code/hmm.py:266\u001b[0m, in \u001b[0;36mHiddenMarkovModel.logprob\u001b[0;34m(self, sentence, corpus)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Integerize the words and tags of the given sentence, which came from the given corpus.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m isent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_integerize_sentence(sentence, corpus)\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43misent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/etc/conda/envs/nlp-class/lib/python3.9/site-packages/typeguard/__init__.py:1033\u001b[0m, in \u001b[0;36mtypechecked.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m memo \u001b[38;5;241m=\u001b[39m _CallMemo(python_func, _localns, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m   1032\u001b[0m check_argument_types(memo)\n\u001b[0;32m-> 1033\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     check_return_type(retval, memo)\n",
      "File \u001b[0;32m~/Desktop/HW6/EN.601.465-HW6/code/hmm.py:309\u001b[0m, in \u001b[0;36mHiddenMarkovModel.forward_pass\u001b[0;34m(self, isent)\u001b[0m\n\u001b[1;32m    303\u001b[0m alpha[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meye[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_t]  \u001b[38;5;66;03m# vector that is one-hot at BOS_TAG\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# Note: once you have this working on the ice cream data, you may\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# have to modify this design slightly to avoid underflow on the\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# English tagging data. See section C in the reading handout.\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m   \u001b[38;5;66;03m# you fill this in!\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_Z\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log.info(\"*** Hidden Markov Model (HMM)\")\n",
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "loss_sup = lambda model: model_cross_entropy(model, eval_corpus=ensup)\n",
    "hmm.train(corpus=ensup, loss=loss_sup, λ=1.0,\n",
    "          save_path=\"ensup_hmm.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's throw in the unsupervised training data as well, and continue\n",
    "training as before, in order to increase the regularized log-likelihood on\n",
    "this larger, semi-supervised training set.  It's now the *incomplete-data*\n",
    "log-likelihood.\n",
    "\n",
    "This time, we'll use a different evaluation loss function: we'll stop when the\n",
    "*tagging error rate* on a held-out dev set stops getting better.  Also, the\n",
    "implementation of this loss function (`viterbi_error_rate`) includes a helpful\n",
    "side effect: it logs the *cross-entropy* on the held-out dataset as well, just\n",
    "for your information.\n",
    "\n",
    "We hope that held-out tagging accuracy will go up for a little bit before it\n",
    "goes down again (see Merialdo 1994). (Log-likelihood on training data will\n",
    "continue to improve, and that improvement may generalize to held-out\n",
    "cross-entropy.  But getting accuracy to increase is harder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ensup_hmm.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hmm \u001b[38;5;241m=\u001b[39m \u001b[43mHiddenMarkovModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mensup_hmm.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# reset to supervised model (in case you're re-executing this bit)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m loss_dev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m model: viterbi_error_rate(model, eval_corpus\u001b[38;5;241m=\u001b[39mendev, \n\u001b[1;32m      3\u001b[0m                                             known_vocab\u001b[38;5;241m=\u001b[39mknown_vocab)\n\u001b[1;32m      4\u001b[0m hmm\u001b[38;5;241m.\u001b[39mtrain(corpus\u001b[38;5;241m=\u001b[39mentrain, loss\u001b[38;5;241m=\u001b[39mloss_dev, λ\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      5\u001b[0m           save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentrain_hmm.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/HW6/EN.601.465-HW6/code/hmm.py:412\u001b[0m, in \u001b[0;36mHiddenMarkovModel.load\u001b[0;34m(cls, path, device)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m): path \u001b[38;5;241m=\u001b[39m Path(path)   \u001b[38;5;66;03m# convert str argument to Path if needed\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# torch.load is similar to pickle.load but handles tensors too\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# map_location allows loading tensors on different device than saved\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType Error: expected object of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    416\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom saved file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/etc/conda/envs/nlp-class/lib/python3.9/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/etc/conda/envs/nlp-class/lib/python3.9/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/etc/conda/envs/nlp-class/lib/python3.9/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ensup_hmm.pkl'"
     ]
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel.load(\"ensup_hmm.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n",
    "loss_dev = lambda model: viterbi_error_rate(model, eval_corpus=endev, \n",
    "                                            known_vocab=known_vocab)\n",
    "hmm.train(corpus=entrain, loss=loss_dev, λ=1.0,\n",
    "          save_path=\"entrain_hmm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also retry the above workflow where you start with a worse supervised\n",
    "model (like Merialdo).  Does EM help more in that case?  It's easiest to rerun\n",
    "exactly the code above, but first make the `ensup` file smaller by copying\n",
    "`ensup-tiny` over it.  `ensup-tiny` is only 25 sentences (that happen to cover\n",
    "all tags in `endev`).  Back up your old `ensup` and your old `*.pkl` models\n",
    "before you do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed look at the first 10 sentences in the held-out corpus,\n",
    "including Viterbi tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_your_data(model, dev, N):\n",
    "    for m, sentence in enumerate(dev):\n",
    "        if m >= N: break\n",
    "        viterbi = model.viterbi_tagging(sentence.desupervise(), endev)\n",
    "        counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                              known_vocab=known_vocab)\n",
    "        num = counts['NUM', 'ALL']\n",
    "        denom = counts['DENOM', 'ALL']\n",
    "        \n",
    "        log.info(f\"Gold:    {sentence}\")\n",
    "        log.info(f\"Viterbi: {viterbi}\")\n",
    "        log.info(f\"Loss:    {denom - num}/{denom}\")\n",
    "        xent = -model.logprob(sentence, endev) / len(sentence)  # measured in nats\n",
    "        log.info(f\"Cross-entropy: {xent/math.log(2)} nats (= perplexity {math.exp(xent)})\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_at_your_data(hmm, endev, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try supervised training of a CRF (this doesn't use the unsupervised\n",
    "part of the data, so it is comparable to the supervised pre-training we did\n",
    "for the HMM).  We will use SGD to approximately maximize the regularized\n",
    "log-likelihood. \n",
    "\n",
    "As with the semi-supervised HMM training, we'll periodically evaluate the\n",
    "tagging accuracy (and also print the cross-entropy) on a held-out dev set.\n",
    "We use the default `eval_interval` and `tolerance`.  If you want to stop\n",
    "sooner, then you could increase the `tolerance` so the training method decides\n",
    "sooner that it has converged.\n",
    "\n",
    "We arbitrarily choose reg = 1.0 for L2 regularization, learning rate = 0.05,\n",
    "and a minibatch size of 10, but it would be better to search for the best\n",
    "value of these hyperparameters.\n",
    "\n",
    "Note that the logger reports the CRF's *conditional* cross-entropy, log p(tags\n",
    "| words) / n.  This is much lower than the HMM's *joint* cross-entropy log\n",
    "p(tags, words) / n, but that doesn't mean the CRF is worse at tagging.  The\n",
    "CRF is just predicting less information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Conditional Random Field (CRF)\\n\")\n",
    "crf = ConditionalRandomField(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "crf.train(corpus=ensup, loss=loss_dev, reg=1.0, lr=0.05, minibatch_size=10,\n",
    "          save_path=\"ensup_crf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how the CRF does on individual sentences. \n",
    "(Do you see any error patterns here that would inspire additional CRF features?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_at_your_data(crf, endev, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
